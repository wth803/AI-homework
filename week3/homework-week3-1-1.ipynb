{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f9949ff-7561-4ba6-82b3-d17aee77bc1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69fb25070bfa475db65327206766483f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/651 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pactera/miniconda3/envs/aistudy5/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\n",
      "/home/pactera/miniconda3/envs/aistudy5/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n",
      "/home/pactera/miniconda3/envs/aistudy5/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float16)\n",
      "CUDA extension not installed.\n",
      "CUDA extension not installed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3374dba8cfc0433c915c10c366e82743",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/251M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "182824d964394645b71f0dec1f978aba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e8f0a438b2b4be2bf53b0d18c88092f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/685 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a39b47043f34be7ba966e6b6c6b40de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c32f8ff9d027465c9e0807dfd457eabc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36da0794409f4a008da875c5010385b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/441 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef363de2d9ad4f8dba453f025719bc6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd6df00d6ee34c82ba06c1a58724641d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/733k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1296a3c17b6e4efc80fad9cb1b3a05ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/6.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65b6b57242ba40c38293ba11e4c4b6bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/657k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "091ee2b354e145f198d578513b987dc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f91bba5c2fd45baace7f87dc64b62be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db9990fbd72c47fd82cd5ab15ec5d544",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "369771571ca443cc969ff1248b2a2dcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing model.decoder.layers blocks :   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b19a93f57fd45e99592e03a6d2c3c64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8d97a610f9347eaa4c3d0c576017124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22e7a1a0e00e4cb2958b7036204eac9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cac3467b69c745f999db187edfee72a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9a8a3565bf54274ad22f097b5c7d7af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f421f6abc10445378965a018cfea31dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05fc15bb31574d8481aec14db30c35e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "252b5103ac254190bad9717577e22fbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e03c129050d54aa2abb3ffb6a2f431fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16b7bc1d7a184b50ae93ea6015f2d429",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6117cfae04104cd8be70c83af337f745",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eb6bdb2fa7a4217809b408f515b6059",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Quantizing layers inside the block:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pactera/miniconda3/envs/aistudy5/lib/python3.11/site-packages/transformers/modeling_utils.py:4481: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 由于资源限制 使用更轻量模型\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GPTQConfig\n",
    "import torch\n",
    "\n",
    "model_name_or_path = \"facebook/opt-125m\"\n",
    "\n",
    "quantization_config = GPTQConfig(\n",
    "     bits=4, \n",
    "     group_size=128,\n",
    "     dataset=\"wikitext2\",\n",
    "     desc_act=False,\n",
    ")\n",
    "quant_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8142300-c517-422c-856d-59cad964b355",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training': True,\n",
       " '_parameters': {},\n",
       " '_buffers': {'qweight': tensor([[ 1711760090, -1248295259, -2025411892,  ..., -1486452502,\n",
       "            2019142072, -1735820810],\n",
       "          [-2000132747,  -578262345,  1484081337,  ..., -1230600537,\n",
       "           -2019252040, -2023311003],\n",
       "          [ -710293851, -1153090188,  1431922298,  ..., -1768449094,\n",
       "            2042194587, -2004125258],\n",
       "          ...,\n",
       "          [-1183500136, -1762945894, -1756001608,  ..., -1788237410,\n",
       "            -394933383,  -679106902],\n",
       "          [-1990626701,  1217620041,  1738299580,  ...,  1740212373,\n",
       "           -2000991594, -1753806216],\n",
       "          [-2010617241,  2023455605,  2005572185,  ..., -1211525767,\n",
       "           -1450403976, -2024523140]], device='cuda:0', dtype=torch.int32),\n",
       "  'qzeros': tensor([[2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071],\n",
       "          [2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071],\n",
       "          [2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071],\n",
       "          [2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071],\n",
       "          [2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071],\n",
       "          [2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071,\n",
       "           2004318071, 2004318071, 2004318071, 2004318071, 2004318071, 2004318071]],\n",
       "         device='cuda:0', dtype=torch.int32),\n",
       "  'scales': tensor([[0.0472, 0.0420, 0.0370,  ..., 0.0180, 0.0131, 0.0191],\n",
       "          [0.0500, 0.0480, 0.0369,  ..., 0.0103, 0.0141, 0.0116],\n",
       "          [0.0428, 0.0522, 0.0415,  ..., 0.0188, 0.0141, 0.0216],\n",
       "          [0.0474, 0.0363, 0.0448,  ..., 0.0124, 0.0148, 0.0114],\n",
       "          [0.0438, 0.0494, 0.0377,  ..., 0.0201, 0.0175, 0.0143],\n",
       "          [0.0471, 0.0477, 0.0385,  ..., 0.0145, 0.0142, 0.0172]],\n",
       "         device='cuda:0', dtype=torch.float16),\n",
       "  'g_idx': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "          2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "          2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "          2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "          2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "          2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "          3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "          3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "          3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "          3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "          3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "          3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "          4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "          4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "          4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "          4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "          4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "          5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "          5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "          5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "          5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "          5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5],\n",
       "         device='cuda:0', dtype=torch.int32),\n",
       "  'bias': tensor([-1.4294e-01, -3.6646e-01, -2.9221e-02, -2.8613e-01, -3.0615e-01,\n",
       "          -2.6880e-01,  7.9773e-02,  8.0261e-02, -1.7957e-01, -5.2887e-02,\n",
       "           1.4319e-01, -1.8982e-01,  3.6694e-01, -6.1554e-02,  3.2617e-01,\n",
       "          -2.8857e-01,  6.8481e-02, -2.7271e-01,  1.5649e-01, -2.1838e-01,\n",
       "          -2.1082e-01, -1.3550e-01, -2.5049e-01, -3.3594e-01,  2.9150e-01,\n",
       "          -7.9773e-02,  1.3440e-01,  8.8196e-02,  3.8110e-01,  1.4294e-01,\n",
       "          -7.5989e-02, -1.9873e-01, -2.1973e-02,  1.3397e-02,  1.1505e-01,\n",
       "           2.5146e-01,  2.8589e-01,  2.9956e-01, -2.2217e-01,  1.3135e-01,\n",
       "          -2.5269e-01, -2.0190e-01,  8.5083e-02, -5.4901e-02,  2.3267e-01,\n",
       "          -1.3831e-01, -5.8624e-02, -1.1206e-01,  5.0488e-01,  1.4526e-01,\n",
       "           5.2295e-01,  2.3157e-01, -1.0779e-01,  2.0374e-01,  1.8323e-01,\n",
       "          -3.3862e-01,  1.9885e-01, -1.6028e-01, -6.7139e-02,  3.3740e-01,\n",
       "           1.3025e-01,  1.1078e-01,  3.5034e-01, -2.9590e-01,  5.2539e-01,\n",
       "           2.8003e-01, -3.3691e-02,  3.4546e-01,  1.3342e-01,  1.0510e-01,\n",
       "          -1.2433e-01,  2.7686e-01, -4.4098e-02,  6.0921e-03,  1.0352e-01,\n",
       "           5.0635e-01, -1.7041e-01,  5.0391e-01, -3.5010e-01,  5.4590e-01,\n",
       "          -1.5039e-01, -1.8030e-01, -1.4600e-01,  3.5498e-01,  1.1182e-01,\n",
       "           2.5659e-01, -3.5352e-01, -3.8379e-01,  5.3174e-01, -2.0215e-01,\n",
       "          -2.5928e-01, -1.6821e-01,  3.0304e-02, -2.7026e-01, -4.4312e-01,\n",
       "          -2.2449e-01,  1.2000e-01,  6.0059e-02,  2.6953e-01,  3.9722e-01,\n",
       "          -5.0195e-01, -4.9927e-02,  6.1920e-02, -1.1469e-01, -1.8988e-03,\n",
       "          -6.6650e-02, -1.6272e-01,  3.7134e-01, -3.2568e-01,  7.4196e-03,\n",
       "           6.5079e-03, -3.2318e-02, -2.0874e-02, -3.4790e-01, -4.3579e-02,\n",
       "           3.3618e-01, -1.6418e-01, -4.5441e-02,  2.7246e-01,  1.1267e-01,\n",
       "          -3.1738e-01, -9.1919e-02, -3.3008e-01,  4.5532e-02,  2.4094e-02,\n",
       "          -2.0227e-01, -1.8384e-01,  9.4910e-02, -5.8167e-02, -2.6343e-01,\n",
       "          -1.4722e-01,  3.2153e-01, -8.6609e-02, -3.0957e-01,  2.8174e-01,\n",
       "          -2.3413e-01,  1.7090e-01,  2.5317e-01,  1.2683e-01, -3.0444e-01,\n",
       "           2.6855e-01,  3.8849e-02,  2.2925e-01,  1.9922e-01,  2.4719e-01,\n",
       "           4.0942e-01,  1.1780e-01,  1.1475e-01, -6.1127e-02,  2.1313e-01,\n",
       "           3.8184e-01, -5.8899e-02,  1.0278e-01,  2.3950e-01,  2.8784e-01,\n",
       "          -3.0457e-02, -1.3049e-01, -3.7646e-01,  2.2119e-01, -1.6406e-01,\n",
       "          -1.9495e-01,  1.7273e-01, -2.5464e-01,  3.6768e-01, -2.9102e-01,\n",
       "          -1.5918e-01,  2.0471e-01, -3.2812e-01,  2.2131e-01, -1.1847e-01,\n",
       "           2.0642e-01,  3.2324e-01, -4.1895e-01, -1.8042e-01, -2.6733e-01,\n",
       "          -2.9834e-01, -2.4695e-01, -2.6465e-01,  2.0676e-02,  6.9763e-02,\n",
       "           1.1255e-01,  3.0957e-01,  5.0293e-01,  3.1421e-01,  1.4185e-01,\n",
       "          -2.6562e-01, -2.4817e-01, -5.6854e-02,  1.8518e-01, -2.5269e-01,\n",
       "          -2.2961e-01, -8.8867e-02, -4.0796e-01, -1.9324e-01,  4.3262e-01,\n",
       "          -3.6523e-01,  4.0016e-03,  1.8518e-01,  2.6416e-01,  2.4414e-01,\n",
       "           9.4177e-02, -1.8005e-01, -1.4246e-01, -2.9395e-01,  3.0396e-01,\n",
       "          -3.4131e-01, -4.0039e-01, -2.5732e-01, -3.2959e-01,  1.6553e-01,\n",
       "           2.8491e-01, -2.8394e-01,  5.0342e-01,  2.0190e-01, -4.0576e-01,\n",
       "           1.8066e-01,  2.8882e-01,  4.3945e-01,  7.6561e-03, -3.0322e-01,\n",
       "           2.8101e-01,  1.8518e-01, -3.6682e-02,  1.5088e-01,  2.3340e-01,\n",
       "          -4.3042e-01, -3.6475e-01, -3.6640e-03, -3.4912e-01, -3.8013e-01,\n",
       "          -4.0436e-02,  3.5474e-01, -4.4434e-01,  5.0830e-01,  3.2373e-01,\n",
       "           2.6807e-01,  2.6416e-01, -4.2505e-01,  2.9956e-01,  2.4902e-01,\n",
       "           2.5269e-01,  3.7646e-01, -4.3091e-01,  3.6304e-01,  1.9238e-01,\n",
       "           1.1406e-02, -9.9258e-03, -2.1826e-01, -4.3396e-02, -3.0566e-01,\n",
       "          -3.7842e-01, -1.7554e-01, -5.0293e-01, -3.7109e-01,  3.6548e-01,\n",
       "          -4.4824e-01,  3.2654e-02, -1.1391e-02,  6.0730e-02,  1.7426e-02,\n",
       "          -1.6431e-01, -8.7219e-02,  1.8884e-01,  7.8552e-02, -4.7821e-02,\n",
       "           9.8724e-03, -1.9165e-02, -9.9945e-03, -1.0797e-01, -4.2084e-02,\n",
       "           3.9635e-03, -2.5732e-01,  1.3252e-02,  7.1289e-02,  1.6113e-02,\n",
       "          -1.3382e-02, -2.1393e-02,  8.0872e-02, -2.5952e-01,  2.6932e-02,\n",
       "          -1.4763e-02, -2.5284e-02, -2.7905e-01, -3.7659e-02, -1.9409e-02,\n",
       "          -8.9050e-02, -9.7733e-03, -6.6223e-02,  3.5400e-02, -7.4654e-03,\n",
       "           6.4697e-02,  1.6052e-01, -1.9073e-02, -1.9806e-02,  3.6835e-02,\n",
       "           6.1737e-02, -1.0675e-01,  1.5793e-02,  1.3147e-01,  2.5772e-02,\n",
       "           3.2501e-02,  3.2440e-02, -4.0619e-02,  4.0359e-03, -5.0140e-02,\n",
       "          -7.7087e-02,  1.2772e-02,  7.0862e-02, -5.2246e-02,  5.9509e-04,\n",
       "          -2.0569e-02, -4.2999e-02,  1.4465e-02, -2.6562e-01,  5.4359e-03,\n",
       "          -9.1858e-02, -6.3171e-02,  1.6586e-02,  4.0649e-02,  8.2214e-02,\n",
       "           2.0844e-02,  5.9967e-03,  4.3488e-02,  2.6047e-02, -2.3384e-03,\n",
       "          -1.2207e-02, -2.7603e-02,  5.6250e-01, -1.6510e-02, -1.4435e-02,\n",
       "           5.2930e-01,  2.1805e-02, -1.0262e-03, -2.6810e-02, -2.4414e-03,\n",
       "          -2.5978e-03,  5.5615e-01, -1.9653e-02, -3.8357e-03, -1.4435e-02,\n",
       "          -1.5656e-02, -5.8441e-02, -1.7746e-02, -2.8870e-02,  4.0314e-02,\n",
       "           7.3671e-04,  2.7588e-02, -2.4231e-02,  1.5434e-02,  7.2289e-03,\n",
       "          -1.5221e-02,  3.0075e-02,  1.4343e-02,  1.7609e-02, -7.1869e-03,\n",
       "          -2.0630e-02, -3.5736e-02,  3.8574e-02,  6.7139e-04,  1.4320e-02,\n",
       "           4.2343e-03, -3.5156e-02,  2.5558e-02,  4.6659e-04, -1.2123e-02,\n",
       "          -2.3880e-02,  1.8906e-02,  7.2937e-03, -2.8259e-02,  1.5465e-02,\n",
       "          -5.1074e-01,  3.4149e-02,  1.1234e-03,  1.4786e-02,  1.7639e-02,\n",
       "           5.1849e-02, -6.3591e-03,  4.0741e-03, -2.8381e-03, -2.1534e-03,\n",
       "          -3.2532e-02,  4.7424e-02,  1.7365e-02,  5.9853e-03,  1.7651e-01,\n",
       "          -6.3477e-02, -3.6133e-02, -1.9006e-01,  2.5586e-01, -3.6938e-01,\n",
       "          -3.5034e-01, -1.9250e-01, -3.6938e-01, -1.6724e-01,  3.4619e-01,\n",
       "           2.4377e-01,  1.3721e-01,  1.8158e-02,  1.2109e-01,  3.1128e-01,\n",
       "          -4.5228e-04,  1.8616e-01,  4.4580e-01,  2.2864e-01, -3.3661e-02,\n",
       "          -2.1655e-01,  4.4507e-01, -1.2964e-01, -1.9812e-01, -2.3779e-01,\n",
       "          -2.4524e-01,  2.4585e-01, -2.5439e-01,  4.7095e-01, -6.5308e-02,\n",
       "          -3.3765e-01, -4.6045e-01, -1.1108e-01,  2.8442e-01,  1.3000e-01,\n",
       "           5.0342e-01,  1.7236e-01,  1.1676e-01,  1.6870e-01,  3.2983e-01,\n",
       "          -2.1204e-01,  1.1945e-01,  9.6313e-02,  1.2390e-01, -3.0811e-01,\n",
       "           3.0981e-01, -7.5623e-02,  1.7664e-01,  3.6572e-01,  4.6582e-01,\n",
       "           1.7114e-01, -2.4744e-01,  7.0496e-02, -3.2837e-02,  3.2568e-01,\n",
       "           1.4648e-01, -2.9785e-01, -1.1273e-01,  1.7529e-01, -3.9429e-01,\n",
       "          -1.6736e-01, -3.0609e-02, -4.4824e-01, -6.6284e-02,  1.2543e-02,\n",
       "          -1.6614e-01,  2.9614e-01, -3.0859e-01, -1.7639e-01,  8.0322e-02,\n",
       "          -3.1348e-01, -2.6514e-01, -2.7295e-01, -2.3755e-01,  1.0217e-01,\n",
       "           8.8379e-02,  8.1177e-02,  3.8391e-02, -5.9509e-02,  3.3325e-01,\n",
       "          -7.1594e-02, -7.3669e-02,  2.3544e-02,  4.9347e-02,  1.7932e-01,\n",
       "           1.3123e-01,  1.7517e-01, -3.6499e-01,  2.6733e-01, -2.6245e-01,\n",
       "          -8.0490e-03,  7.1487e-03, -2.1204e-01,  5.1123e-01, -1.5356e-01,\n",
       "          -4.1382e-01, -1.7810e-01, -6.2866e-02,  1.1420e-01,  8.2520e-02,\n",
       "          -3.1641e-01, -1.5335e-02, -5.0995e-02, -2.9199e-01, -1.7847e-01,\n",
       "           4.2145e-02,  2.9663e-01, -7.2144e-02, -1.3220e-01,  3.0322e-01,\n",
       "          -1.2683e-01, -2.8961e-02,  2.2430e-02,  7.2876e-02,  7.6599e-02,\n",
       "          -2.7145e-02,  2.5854e-01, -1.2128e-01, -3.8867e-01, -6.0699e-02,\n",
       "           2.7786e-02, -3.5059e-01,  6.4636e-02,  1.3660e-01,  2.9395e-01,\n",
       "          -1.2329e-01, -8.0444e-02, -9.2163e-02, -5.5518e-01,  1.5649e-01,\n",
       "          -5.0732e-01, -5.2539e-01,  5.5127e-01, -2.2064e-02, -6.2158e-01,\n",
       "           5.0586e-01, -5.2490e-01,  1.0199e-01, -2.4683e-01,  5.1807e-01,\n",
       "           3.6499e-01, -5.5615e-01,  5.6299e-01, -1.1469e-01, -5.1172e-01,\n",
       "           1.5137e-01,  5.5811e-01, -5.7861e-01, -5.0928e-01,  3.2617e-01,\n",
       "           2.3938e-01, -5.7666e-01,  1.6016e-01, -2.5488e-01, -3.4863e-01,\n",
       "           2.4780e-01,  3.9337e-02, -1.6858e-01, -8.2825e-02, -3.3325e-02,\n",
       "           5.0146e-01,  2.9688e-01,  1.3110e-01,  5.7812e-01,  2.0154e-01,\n",
       "           8.7585e-02, -5.1416e-01, -2.4902e-01, -1.6833e-01,  5.3760e-01,\n",
       "          -5.3857e-01,  5.6915e-02,  5.0195e-01,  1.3464e-01,  5.0586e-01,\n",
       "           6.7383e-02,  1.7346e-01, -2.5781e-01, -5.3857e-01,  7.5455e-03,\n",
       "          -5.0781e-01, -3.9404e-01,  3.0176e-01,  5.0977e-01, -1.2421e-01,\n",
       "          -5.0293e-01, -1.2347e-01,  5.1416e-01, -5.3320e-01, -5.7080e-01,\n",
       "          -3.4644e-01,  5.0781e-01, -7.8796e-02, -4.2236e-02, -2.6221e-01,\n",
       "           1.0114e-01,  1.0971e-02, -7.5562e-02, -2.2595e-01, -1.4502e-01,\n",
       "           3.7671e-01,  3.6938e-01,  1.5576e-01,  5.8632e-03,  1.6602e-01,\n",
       "           2.8955e-01, -4.7217e-01, -1.1194e-01,  2.9077e-01,  2.6758e-01,\n",
       "          -6.3293e-02,  1.1340e-01,  5.3174e-01,  4.0210e-01,  1.3965e-01,\n",
       "           1.1505e-01,  8.7891e-02, -8.8562e-02, -4.9023e-01,  3.9380e-01,\n",
       "           1.3611e-01,  2.3767e-01,  1.9424e-02, -5.5322e-01,  2.3218e-01,\n",
       "          -5.2881e-01, -1.0211e-01,  2.5854e-01, -3.2520e-01,  1.6632e-02,\n",
       "          -2.7979e-01,  2.8857e-01, -7.1289e-02, -5.2979e-01,  2.5195e-01,\n",
       "          -3.1226e-01, -2.7466e-03,  1.2793e-01,  9.7168e-02,  6.2103e-02,\n",
       "           9.3567e-02,  2.4023e-01,  4.6069e-01, -5.0926e-03,  2.7271e-01,\n",
       "           1.3184e-01, -2.3901e-01, -3.5205e-01, -1.8982e-01,  3.2806e-02,\n",
       "           1.8774e-01, -1.1591e-01, -5.9021e-02,  2.8702e-02, -3.8379e-01,\n",
       "           1.9470e-01,  2.2986e-01,  1.3220e-01,  1.9446e-01,  3.7292e-02,\n",
       "           1.5076e-01, -6.5308e-02, -3.5461e-02, -7.5867e-02,  1.3379e-01,\n",
       "           1.0284e-01, -2.7740e-02, -4.9324e-03,  1.3000e-01, -9.5032e-02,\n",
       "           2.3041e-02,  4.3884e-02,  1.4294e-01, -5.5054e-02,  6.8909e-02,\n",
       "           5.5939e-02,  6.2408e-02, -2.1582e-01,  2.5000e-01,  5.3076e-01,\n",
       "          -2.2791e-01, -2.5049e-01,  1.6138e-01, -9.7229e-02, -2.1667e-01,\n",
       "          -2.4976e-01, -2.5146e-01, -5.2643e-02, -1.2457e-01, -7.0229e-03,\n",
       "          -1.5002e-01, -8.4473e-02, -9.4681e-03,  8.1055e-02,  9.6252e-02,\n",
       "           1.7773e-01, -8.1604e-02,  1.3806e-01, -1.8677e-01,  1.6895e-01,\n",
       "          -3.7628e-02,  5.9662e-02, -9.2773e-02, -7.1533e-02, -1.9885e-01,\n",
       "           5.9174e-02,  2.5000e-01, -2.3514e-02,  7.3608e-02,  2.2290e-01,\n",
       "           6.6772e-02, -1.2500e-01, -1.3550e-01, -6.0028e-02, -1.3147e-01,\n",
       "          -1.2756e-01,  6.3416e-02,  1.3062e-01,  1.2488e-01,  6.1584e-02,\n",
       "          -6.7101e-03, -3.3875e-02, -1.6556e-02,  1.8738e-02, -4.7836e-03,\n",
       "          -3.2776e-02,  1.4427e-02, -5.1221e-01, -4.6661e-02,  3.5343e-03,\n",
       "           1.5762e-02,  2.3918e-03, -3.1250e-02, -1.8661e-02, -3.0685e-02,\n",
       "           1.5762e-02, -8.2779e-03, -7.0190e-02,  1.4915e-02, -1.6663e-02,\n",
       "           2.6138e-02, -8.9645e-03,  1.2150e-03, -1.5222e-01, -2.7023e-02,\n",
       "          -2.4277e-02, -8.3237e-03,  2.5192e-02, -7.4120e-03, -7.1487e-03,\n",
       "          -3.6377e-01,  7.0496e-02, -2.0630e-01,  2.1744e-02,  2.2049e-02,\n",
       "          -1.7487e-02, -5.1910e-02,  3.1799e-02,  1.8585e-02,  2.3880e-02,\n",
       "          -1.8433e-02,  9.0866e-03,  1.8753e-02,  1.7273e-02, -4.1580e-03,\n",
       "           1.5976e-02, -2.8519e-02,  4.3030e-03, -2.9312e-02,  3.5217e-02,\n",
       "           4.0771e-02, -1.8188e-01,  9.0103e-03,  5.0586e-01,  2.0020e-02,\n",
       "           1.9806e-02,  1.6861e-02,  2.5436e-02,  1.7548e-02, -8.7357e-03,\n",
       "          -1.6785e-02,  1.8906e-02, -1.3695e-02], device='cuda:0',\n",
       "         dtype=torch.float16)},\n",
       " '_non_persistent_buffers_set': set(),\n",
       " '_backward_pre_hooks': OrderedDict(),\n",
       " '_backward_hooks': OrderedDict(),\n",
       " '_is_full_backward_hook': None,\n",
       " '_forward_hooks': OrderedDict(),\n",
       " '_forward_hooks_with_kwargs': OrderedDict(),\n",
       " '_forward_hooks_always_called': OrderedDict(),\n",
       " '_forward_pre_hooks': OrderedDict(),\n",
       " '_forward_pre_hooks_with_kwargs': OrderedDict(),\n",
       " '_state_dict_hooks': OrderedDict(),\n",
       " '_state_dict_pre_hooks': OrderedDict(),\n",
       " '_load_state_dict_pre_hooks': OrderedDict(),\n",
       " '_load_state_dict_post_hooks': OrderedDict(),\n",
       " '_modules': {},\n",
       " 'infeatures': 768,\n",
       " 'outfeatures': 768,\n",
       " 'bits': 4,\n",
       " 'group_size': 128,\n",
       " 'maxq': 15,\n",
       " 'half_indim': 384,\n",
       " 'use_cuda_fp16': True,\n",
       " 'wf': tensor([[ 0,  4,  8, 12, 16, 20, 24, 28]], dtype=torch.int32),\n",
       " 'kernel_switch_threshold': 128,\n",
       " 'autogptq_cuda_available': False,\n",
       " 'autogptq_cuda': None,\n",
       " 'trainable': False,\n",
       " 'device': device(type='cuda', index=0)}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_model.model.decoder.layers[0].self_attn.q_proj.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a35332f4-0b1c-4926-a87f-bd7df930b005",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_model.save_pretrained(\"models/opt-opt-125m-gptq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffce3bf0-5c10-4ce6-ae37-0c01ff3707aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good Moring teacher! I am sorry you are going through this. I hope you find some peace in your life.\n",
      "Thank you! I'm glad you're here. I'm glad you're here.\n"
     ]
    }
   ],
   "source": [
    "# 生成文本\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "\n",
    "text = \"Good Moring teacher! I am sorry\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(0)\n",
    "\n",
    "out = quant_model.generate(**inputs, max_new_tokens=64)\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19688f40-ff77-4a11-aac5-f52a21cccd22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am so happy to hear that you are doing well. I am so glad you are doing well. I hope you are doing well.\n",
      "Thank you so much! I'm glad you're doing well. I'm glad you're doing well.\n"
     ]
    }
   ],
   "source": [
    "text = \"I am so happy to hear that\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(0)\n",
    "\n",
    "out = quant_model.generate(**inputs, max_new_tokens=64)\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe16f58-15a9-4b52-bca8-8c4897799501",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
